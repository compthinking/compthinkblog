---
title: The Terminator Question, A Rant
tags:
  - machine-learning
  - artificial-inteligence
  - generative-models
  - large-language-models
  - ai-doom
show_tags: true
show_bib: false
status: draft
---
(*This orginated as a rant aside while writing the post [[2024-01-06-Liveblog-Movie-The-Creator]] and I thought better of posting it. Perhaps I'll use it some day, it's proper snark.*)

The obsession of journalists, and even many researchers with always asking "The Terminator" question, is already very tired. Not that this isn't an important topic, what could be more important than the survival of Human-ity IT. SELF?

Well, survival of any significant portion of the worlds species is pretty important. As is the livability of the climate, the level of suffering of millions, or billions of humans now and in future centuries. *Assuming* we get past the AGI Bottleneck of course. 

Which I *do* assume. Human beings are many things, most of them not great, but it is certainly that we are one of the most, if not the most, adaptable, resilient and relentless species that this planet, and as far as we know, the Universe, has ever created. As a species, we'll survive somehow. 

But in the meantime, how much suffering will there be if we take our eye off the many balls in the air right now because of what is, I think, an *unfounded fear*. To put it in a quite flippant way, it is because of a surprisingly good chatbot, we now suddenly think that we are going to  have self-aware robots in a couple years, and soon after, inevitably for some reason, we'll be [paper-clipped out of existence by some super-intelligent optimization function](https://en.wikipedia.org/wiki/Instrumental_convergence#Paperclip_maximizer)? And I know, it doesn't even need to be inevitable, it only need to be 1/10,000th of a percent chance, isn't it worth taking seriously? People have been trying to make *that* case on climate change for 50 years and it still hasn't sunk in, and look where we are now, we're in the middle of a catastrophe. But climate has always been very clear, and on solid ground because it's *physics*. We know what will happen. The fears of an AIApocalypse are much less well understand, less well-founded, and draw *mostly* from existing fears of change, status quo interests, and invalid logical reasoning.

`</endOfRant>`
